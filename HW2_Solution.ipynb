{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0Adld_PdGAz"
   },
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9jKpXk6Bfuls"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwQzPZjsdQMg"
   },
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raB7lImGc9yL"
   },
   "outputs": [],
   "source": [
    "class Quotes:\n",
    "  \n",
    "    base_url = \"http://quotes.toscrape.com\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "    self.url = url\n",
    "    self.get_response()\n",
    "\n",
    "    def get_response(self):\n",
    "    page = requests.get(self.url)\n",
    "    if page.status_code == 200:\n",
    "      self.response = TextResponse(body=page.text,url=page.url,encoding=\"utf-8\")\n",
    "    else:\n",
    "      self.response = None\n",
    "    return self.response\n",
    "\n",
    "    def get_data(self):\n",
    "    if self.response:\n",
    "        return {\n",
    "          \"quote\":self.response.css(\"span.text::text\").extract(),\n",
    "          \"author\":self.response.css(\"small.author::text\").extract(),\n",
    "          \"author_page\":[self.base_url+i for i in self.response.css(\"small.author ~ a::attr(href)\").extract()],\n",
    "          \"tags\":[i.css(\"a.tag::text\").extract() for i in self.response.css(\"div.tags\")]\n",
    "      }\n",
    "\n",
    "    def get_next_pagelink(self):\n",
    "    if self.response:\n",
    "        next_button = self.response.css(\"li.next > a::attr(href)\").extract_first()\n",
    "        if next_button:\n",
    "        return self.base_url + next_button\n",
    "        else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Xf8CT98f7hx"
   },
   "outputs": [],
   "source": [
    "url = \"http://quotes.toscrape.com/page/1/\"\n",
    "SLEEP = 3\n",
    "PAGES = []\n",
    "while True:\n",
    "  page = Quotes(url)\n",
    "  PAGES.append(pd.DataFrame(page.get_data()))\n",
    "  next_page = page.get_next_pagelink()\n",
    "  print(f\"Finished scraping page {url.split('/')[-2]}\")\n",
    "  if next_page:\n",
    "    url = next_page\n",
    "    time.sleep(SLEEP)\n",
    "  else:\n",
    "    print(\"No more page found, stoping scraper\")\n",
    "    break\n",
    "\n",
    "data = pd.concat(PAGES)\n",
    "assert data.shape == (100,4)\n",
    "data.to_csv(\"all_quotes.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UE3E6pV1spyt"
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NJCpE3PreGm"
   },
   "outputs": [],
   "source": [
    "class Movies:\n",
    "  \n",
    "  def __init__(self, url):\n",
    "    self.url = url\n",
    "    self.get_response()\n",
    "\n",
    "  def get_response(self):\n",
    "    page = requests.get(self.url)\n",
    "    if page.status_code == 200:\n",
    "      self.response = TextResponse(body=page.text,url=page.url,encoding=\"utf-8\")\n",
    "    else:\n",
    "      self.response = None\n",
    "    return self.response\n",
    "\n",
    "  def get_data(self):\n",
    "    return {\n",
    "        \"title\": self.response.css(\"td.titleColumn > a::text\").extract(),\n",
    "        \"year\": self.response.css(\"td.titleColumn > span.secondaryInfo::text\").re(\"\\((\\d{4})\\)\"),\n",
    "        \"ranking\": self.response.xpath(\"//td[@class='titleColumn']/div[@class='velocity']/text()[1]\").re(\"\\d+\"),\n",
    "        \"rating\": [i.css(\"strong::text\").extract_first() for i in self.response.css(\"td[class*='imdbRating']\")],\n",
    "        \"hyperlink\": self.response.css(\"td.titleColumn > a::attr(href)\").extract()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1R3mqzLuic7"
   },
   "outputs": [],
   "source": [
    "movies = Movies(\"https://www.imdb.com/chart/moviemeter/\")\n",
    "data = movies.get_data()\n",
    "assert np.var([len(i) for i in data.values()])==0\n",
    "data = pd.DataFrame(data)\n",
    "assert data.shape==(100,5)\n",
    "data.to_csv(\"top_movies.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzSFt6hI1Jr6"
   },
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HIlAh-uvCHi"
   },
   "outputs": [],
   "source": [
    "class Books:\n",
    "  \n",
    "  base_url = \"http://books.toscrape.com/catalogue/\"\n",
    "\n",
    "  def __init__(self, url):\n",
    "    self.url = url\n",
    "    self.get_response()\n",
    "\n",
    "  def get_response(self):\n",
    "    page = requests.get(self.url)\n",
    "    if page.status_code == 200:\n",
    "      self.response = TextResponse(body=page.text,url=page.url,encoding=\"utf-8\")\n",
    "    else:\n",
    "      self.response = None\n",
    "    return self.response\n",
    "\n",
    "  def get_data(self):\n",
    "    return {\n",
    "        \"title\": self.response.css(\"a[title]::attr(title)\").extract(),\n",
    "        \"price\": self.response.css(\"p.price_color::text\").re(\"\\d+\\.\\d+\"),\n",
    "        \"rating\": [i.split(\" \")[-1] for i in self.response.css(\"p[class^='star-rating']::attr(class)\").extract()],\n",
    "        \"availability\": self.response.xpath(\"//p[contains(@class,'stock')]/text()[2]\").re(\"\\w+.+\\w\"),\n",
    "        \"hyperlink\": [self.base_url+i for i in self.response.css(\"a[title]::attr(href)\").extract()]\n",
    "        }\n",
    "\n",
    "  def get_next_pagelink(self):\n",
    "    if self.response:\n",
    "      next_button = self.response.css(\"li.next > a::attr(href)\").extract_first()\n",
    "      if next_button:\n",
    "        return self.base_url + next_button\n",
    "      else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FtGOcgpt2ht1"
   },
   "outputs": [],
   "source": [
    "url = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "SLEEP = 3\n",
    "PAGES = []\n",
    "while True:\n",
    "  page = Books(url)\n",
    "  PAGES.append(pd.DataFrame(page.get_data()))\n",
    "  next_page = page.get_next_pagelink()\n",
    "  page_num = re.findall('\\d+',url)[0]\n",
    "  print(f\"Finished scraping page {page_num}\")\n",
    "  if next_page:\n",
    "    url = next_page\n",
    "    time.sleep(SLEEP)\n",
    "  else:\n",
    "    print(\"No more page found, stoping scraper\")\n",
    "    break\n",
    "\n",
    "data = pd.concat(PAGES)\n",
    "assert data.shape == (20*50,5)\n",
    "data.to_csv(\"all_books.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW2-Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
